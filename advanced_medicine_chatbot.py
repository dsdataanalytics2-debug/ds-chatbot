#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üíä ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶Æ‡ßá‡¶°‡¶ø‡¶∏‡¶ø‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü - ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®
PDF, Word, Excel, API ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶® ‡¶∏‡¶π
"""

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import warnings
import requests
import json
from pathlib import Path
import io
import base64
from datetime import datetime
warnings.filterwarnings('ignore')

# PDF ‡¶è‡¶¨‡¶Ç Word ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
try:
    import PyPDF2
    import docx
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    st.warning("PDF/Word ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø PyPDF2 ‡¶è‡¶¨‡¶Ç python-docx ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®")

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class AdvancedMedicineChatbot:
    def __init__(self, excel_file):
        self.excel_file = excel_file
        self.data = None
        self.vectorizer = None
        self.tfidf_matrix = None
        self.pdf_data = []
        self.word_data = []
        self.excel_data = []
        self.api_data = []
        self.all_sources = []
        self.bengali_stop_words = set([
            '‡¶è‡¶¨‡¶Ç', '‡¶Ö‡¶•‡¶¨‡¶æ', '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ', '‡¶Ø‡¶¶‡¶ø', '‡¶§‡¶¨‡ßá', '‡¶ï‡ßá‡¶®', '‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá', '‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º', '‡¶ï‡¶ñ‡¶®', 
            '‡¶ï‡¶ø', '‡¶ï‡ßã‡¶®', '‡¶ï‡¶æ‡¶¶‡ßá‡¶∞', '‡¶ï‡¶æ‡¶∞', '‡¶ï‡¶æ‡¶ï‡ßá', '‡¶ï‡¶ø', '‡¶ï‡¶ø', '‡¶ï‡¶ø', '‡¶ï‡¶ø', '‡¶ï‡¶ø', 
            '‡¶π‡¶Ø‡¶º', '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶π‡¶¨‡ßá', '‡¶ï‡¶∞‡¶§‡ßá', '‡¶ï‡¶∞‡ßá', '‡¶ï‡¶∞‡¶¨‡ßá', '‡¶Ü‡¶õ‡ßá', '‡¶®‡ßá‡¶á', '‡¶•‡¶æ‡¶ï‡¶¨‡ßá',
            '‡¶è‡¶ü‡¶æ', '‡¶è‡¶ü‡¶ø', '‡¶∏‡ßá‡¶ü‡¶æ', '‡¶∏‡ßá‡¶ü‡¶ø', '‡¶è‡¶á', '‡¶∏‡ßá‡¶á', '‡¶Ø‡ßá', '‡¶Ø‡¶æ', '‡¶Ø‡¶æ‡¶∞', '‡¶Ø‡¶æ‡¶¶‡ßá‡¶∞',
            '‡¶Ü‡¶Æ‡¶ø', '‡¶Ü‡¶Æ‡¶∞‡¶æ', '‡¶§‡ßÅ‡¶Æ‡¶ø', '‡¶§‡ßã‡¶Æ‡¶∞‡¶æ', '‡¶∏‡ßá', '‡¶§‡¶æ‡¶∞‡¶æ', '‡¶Ü‡¶™‡¶®‡¶ø', '‡¶Ü‡¶™‡¶®‡¶æ‡¶∞‡¶æ',
            '‡¶è‡¶ñ‡¶æ‡¶®‡ßá', '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º', '‡¶ï‡ßã‡¶•‡¶æ‡¶ì', '‡¶ï‡ßã‡¶•‡¶æ‡¶ì', '‡¶ï‡ßã‡¶•‡¶æ‡¶ì',
            '‡¶è‡¶ñ‡¶®', '‡¶§‡¶ñ‡¶®', '‡¶ï‡¶ñ‡¶®', '‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º', '‡¶ï‡¶ñ‡¶®‡¶ì', '‡¶ï‡¶ñ‡¶®‡¶ì', '‡¶ï‡¶ñ‡¶®‡¶ì', '‡¶ï‡¶ñ‡¶®‡¶ì',
            '‡¶≠‡¶æ‡¶≤‡ßã', '‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™', '‡¶¨‡¶°‡¶º', '‡¶õ‡ßã‡¶ü', '‡¶®‡¶§‡ßÅ‡¶®', '‡¶™‡ßÅ‡¶∞‡¶æ‡¶®‡ßã', '‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞', '‡¶ï‡ßÅ‡ßé‡¶∏‡¶ø‡¶§',
            '‡¶∏‡¶π‡¶ú', '‡¶ï‡¶†‡¶ø‡¶®', '‡¶¶‡ßç‡¶∞‡ßÅ‡¶§', '‡¶ß‡ßÄ‡¶∞', '‡¶ó‡¶∞‡¶Æ', '‡¶†‡¶æ‡¶®‡ßç‡¶°‡¶æ', '‡¶â‡¶∑‡ßç‡¶£', '‡¶∂‡ßÄ‡¶§‡¶≤'
        ])
        self.load_data()
        self.preprocess_data()
        
def save_uploaded_file_to_data_source(uploaded_file):
    """UploadedFile ‡¶°‡¶ø‡¶∏‡ßç‡¶ï‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®: ./data source/<timestamp>_<filename>"""
    try:
        data_dir = Path("data source")
        data_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = uploaded_file.name
        dest_path = data_dir / f"{timestamp}_{safe_name}"
        try:
            bytes_data = uploaded_file.getbuffer()
            with open(dest_path, "wb") as f:
                f.write(bytes_data)
        except Exception:
            content = uploaded_file.read()
            with open(dest_path, "wb") as f:
                f.write(content)
            try:
                uploaded_file.seek(0)
            except Exception:
                pass
        return str(dest_path)
    except Exception as e:
        st.warning(f"‡¶´‡¶æ‡¶á‡¶≤ ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ: {e}")
        return ""

    def load_data(self):
        """Excel ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®"""
        try:
            self.data = pd.read_excel(self.excel_file)
            st.success(f"‚úÖ ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶≤‡ßã‡¶° ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá! ‡¶Æ‡ßã‡¶ü {len(self.data)} ‡¶ü‡¶ø ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ó‡ßá‡¶õ‡ßá‡•§")
        except Exception as e:
            st.error(f"‚ùå ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá: {str(e)}")
            return None
    
    def add_pdf_file(self, pdf_file):
        """PDF ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®"""
        if not PDF_AVAILABLE:
            st.error("‚ùå PDF ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶® ‡¶®‡ßá‡¶á‡•§ PyPDF2 ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")
            return False
            
        try:
            saved_path = save_uploaded_file_to_data_source(pdf_file)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text_content = ""
            
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text_content += page.extract_text() + "\n"
            
            if text_content.strip():
                self.pdf_data.append({
                    'filename': pdf_file.name,
                    'content': text_content,
                    'source': 'PDF',
                    'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'saved_path': saved_path
                })
                st.success(f"‚úÖ PDF ‡¶´‡¶æ‡¶á‡¶≤ '{pdf_file.name}' ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ø‡ßã‡¶ó ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá")
                return True
            else:
                st.warning("‚ö†Ô∏è PDF ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡ßã‡¶® ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø")
                return False
                
        except Exception as e:
            st.error(f"‚ùå PDF ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ: {str(e)}")
            return False
    
    def add_word_file(self, word_file):
        """Word ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®"""
        if not PDF_AVAILABLE:
            st.error("‚ùå Word ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶® ‡¶®‡ßá‡¶á‡•§ python-docx ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")
            return False
            
        try:
            saved_path = save_uploaded_file_to_data_source(word_file)
            doc = docx.Document(word_file)
            text_content = ""
            
            for paragraph in doc.paragraphs:
                text_content += paragraph.text + "\n"
            
            if text_content.strip():
                self.word_data.append({
                    'filename': word_file.name,
                    'content': text_content,
                    'source': 'Word',
                    'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'saved_path': saved_path
                })
                st.success(f"‚úÖ Word ‡¶´‡¶æ‡¶á‡¶≤ '{word_file.name}' ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ø‡ßã‡¶ó ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá")
                return True
            else:
                st.warning("‚ö†Ô∏è Word ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡ßã‡¶® ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø")
                return False
                
        except Exception as e:
            st.error(f"‚ùå Word ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ: {str(e)}")
            return False
    
    def add_excel_file(self, excel_file):
        """Excel ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®"""
        try:
            saved_path = save_uploaded_file_to_data_source(excel_file)
            df = pd.read_excel(excel_file)
            
            if len(df) > 0:
                # Excel ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
                text_content = ""
                for idx, row in df.iterrows():
                    row_text = " ".join([str(val) for val in row.values if pd.notna(val)])
                    text_content += row_text + "\n"
                
                self.excel_data.append({
                    'filename': excel_file.name,
                    'content': text_content,
                    'dataframe': df,
                    'source': 'Excel',
                    'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'saved_path': saved_path
                })
                st.success(f"‚úÖ Excel ‡¶´‡¶æ‡¶á‡¶≤ '{excel_file.name}' ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ø‡ßã‡¶ó ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá ({len(df)} ‡¶∏‡¶æ‡¶∞‡¶ø)")
                return True
            else:
                st.warning("‚ö†Ô∏è Excel ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶ï‡ßã‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶®‡ßá‡¶á")
                return False
                
        except Exception as e:
            st.error(f"‚ùå Excel ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ: {str(e)}")
            return False
    
    def add_api_data(self, api_url, api_key=None):
        """API ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßÅ‡¶®"""
        try:
            headers = {}
            if api_key:
                headers['Authorization'] = f'Bearer {api_key}'
                headers['X-API-Key'] = api_key
            
            response = requests.get(api_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            # API ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
            text_content = json.dumps(data, indent=2, ensure_ascii=False)
            
            self.api_data.append({
                'url': api_url,
                'content': text_content,
                'raw_data': data,
                'source': 'API',
                'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'status': 'success'
            })
            
            st.success(f"‚úÖ API ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ø‡ßã‡¶ó ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá")
            return True
            
        except requests.exceptions.RequestException as e:
            st.error(f"‚ùå API ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ: {str(e)}")
            return False
        except json.JSONDecodeError:
            st.error("‚ùå API ‡¶•‡ßá‡¶ï‡ßá JSON ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø")
            return False
        except Exception as e:
            st.error(f"‚ùå API ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ: {str(e)}")
            return False
    
    def update_all_sources(self):
        """‡¶∏‡¶¨ ‡¶â‡ßé‡¶∏‡ßá‡¶∞ ‡¶°‡ßá‡¶ü‡¶æ ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶§ ‡¶ï‡¶∞‡ßÅ‡¶®"""
        self.all_sources = []
        
        # ‡¶Æ‡ßÇ‡¶≤ Excel ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
        if self.data is not None:
            for idx, row in self.data.iterrows():
                self.all_sources.append({
                    'source': 'Main Excel',
                    'filename': self.excel_file,
                    'content': ' '.join([str(val) for val in row.values if pd.notna(val)]),
                    'row_index': idx,
                    'data': row.to_dict()
                })
        
        # ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ Excel ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
        for excel_item in self.excel_data:
            for idx, row in excel_item['dataframe'].iterrows():
                self.all_sources.append({
                    'source': 'Uploaded Excel',
                    'filename': excel_item['filename'],
                    'content': ' '.join([str(val) for val in row.values if pd.notna(val)]),
                    'row_index': idx,
                    'data': row.to_dict(),
                    'upload_time': excel_item['upload_time']
                })
        
        # PDF ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
        for pdf_item in self.pdf_data:
            self.all_sources.append({
                'source': 'PDF',
                'filename': pdf_item['filename'],
                'content': pdf_item['content'],
                'upload_time': pdf_item['upload_time']
            })
        
        # Word ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
        for word_item in self.word_data:
            self.all_sources.append({
                'source': 'Word',
                'filename': word_item['filename'],
                'content': word_item['content'],
                'upload_time': word_item['upload_time']
            })
        
        # API ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
        for api_item in self.api_data:
            self.all_sources.append({
                'source': 'API',
                'url': api_item['url'],
                'content': api_item['content'],
                'upload_time': api_item['upload_time']
            })
    
    def preprocess_data(self):
        """‡¶∏‡¶æ‡¶∞‡ßç‡¶ö‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®"""
        if self.data is None:
            return
            
        # ‡¶∏‡¶¨ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
        text_columns = []
        for col in self.data.columns:
            if self.data[col].dtype == 'object':  # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶ï‡¶≤‡¶æ‡¶Æ
                text_columns.append(col)
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ì‡¶∑‡ßÅ‡¶ß‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶Æ‡ßç‡¶Æ‡¶ø‡¶≤‡¶ø‡¶§ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
        self.data['combined_text'] = self.data[text_columns].fillna('').astype(str).agg(' '.join, axis=1)
        
        # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
        self.data['cleaned_text'] = self.data['combined_text'].apply(self.clean_text)
        
        # TF-IDF ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶æ‡¶á‡¶ú‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
        self.vectorizer = TfidfVectorizer(
            max_features=2000,
            ngram_range=(1, 3),
            min_df=1,
            stop_words=None  # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡ßç‡¶ü‡¶™ ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶°‡¶∏ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡¶ø ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶≤ ‡¶ï‡¶∞‡¶¨
        )
        
        # TF-IDF ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
        self.tfidf_matrix = self.vectorizer.fit_transform(self.data['cleaned_text'])
    
    def clean_text(self, text):
        """‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®"""
        # ‡¶≤‡ßã‡¶Ø‡¶º‡¶æ‡¶∞‡¶ï‡ßá‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®
        text = text.lower()
        
        # ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶∞‡¶æ‡¶® ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®
        text = re.sub(r'[^\w\s\u0980-\u09FF]', ' ', text)
        
        # ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶∏‡ßç‡¶™‡ßá‡¶∏ ‡¶∏‡¶∞‡¶æ‡¶®
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡ßç‡¶ü‡¶™ ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶°‡¶∏ ‡¶∏‡¶∞‡¶æ‡¶®
        words = text.split()
        filtered_words = [word for word in words if word not in self.bengali_stop_words]
        
        return ' '.join(filtered_words)
    
    def search_all_sources(self, query, top_k=5):
        """‡¶∏‡¶¨ ‡¶â‡ßé‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö ‡¶ï‡¶∞‡ßÅ‡¶®"""
        self.update_all_sources()
        
        if not self.all_sources:
            return []
        
        results = []
        
        for source in self.all_sources:
            # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
            cleaned_content = self.clean_text(source['content'])
            cleaned_query = self.clean_text(query)
            
            # ‡¶∏‡¶∞‡¶≤ ‡¶ï‡ßÄ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶° ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö‡¶ø‡¶Ç
            query_words = cleaned_query.split()
            content_words = cleaned_content.split()
            
            # ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶ó‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
            matches = sum(1 for word in query_words if word in content_words)
            if matches > 0:
                score = matches / len(query_words)
                
                # ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
                context = self.extract_context(source['content'], query, 200)
                
                results.append({
                    'source': source['source'],
                    'filename': source.get('filename', source.get('url', 'Unknown')),
                    'score': score,
                    'context': context,
                    'full_content': source['content'][:500] + "..." if len(source['content']) > 500 else source['content'],
                    'upload_time': source.get('upload_time', ''),
                    'data': source.get('data', {})
                })
        
        # ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶∏‡¶æ‡¶ú‡¶æ‡¶®
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def extract_context(self, text, query, context_length=200):
        """‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶∞‡¶ø‡¶∞ ‡¶Ü‡¶∂‡ßá‡¶™‡¶æ‡¶∂‡ßá‡¶∞ ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®"""
        query_lower = query.lower()
        text_lower = text.lower()
        
        # ‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶∞‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
        start_pos = text_lower.find(query_lower)
        
        if start_pos == -1:
            # ‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶∞‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
            query_words = query_lower.split()
            for word in query_words:
                pos = text_lower.find(word)
                if pos != -1:
                    start_pos = pos
                    break
        
        if start_pos == -1:
            return text[:context_length] + "..." if len(text) > context_length else text
        
        # ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
        start = max(0, start_pos - context_length // 2)
        end = min(len(text), start_pos + len(query) + context_length // 2)
        
        context = text[start:end]
        
        if start > 0:
            context = "..." + context
        if end < len(text):
            context = context + "..."
        
        return context
    
    def search_medicines(self, query, top_k=5):
        """‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®"""
        if self.data is None or self.vectorizer is None:
            return []
        
        # ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
        cleaned_query = self.clean_text(query)
        
        # ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡¶ï‡ßá TF-IDF ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
        query_vector = self.vectorizer.transform([cleaned_query])
        
        # ‡¶∏‡¶ø‡¶Æ‡¶ø‡¶≤‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø ‡¶ó‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        
        # ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶™‡¶æ‡¶®
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.05:  # ‡¶®‡ßç‡¶Ø‡ßÇ‡¶®‡¶§‡¶Æ ‡¶∏‡¶ø‡¶Æ‡¶ø‡¶≤‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡ßá‡¶∂‡¶π‡ßã‡¶≤‡ßç‡¶°
                medicine_info = self.data.iloc[idx].to_dict()
                medicine_info['similarity_score'] = similarities[idx]
                results.append(medicine_info)
        
        return results
    
    def get_medicine_details(self, medicine_name):
        """‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ì‡¶∑‡ßÅ‡¶ß‡ßá‡¶∞ ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶§‡¶•‡ßç‡¶Ø ‡¶™‡¶æ‡¶®"""
        if self.data is None:
            return None
        
        # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶∏‡¶†‡¶ø‡¶ï ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
        exact_match = self.data[self.data.iloc[:, 0].str.contains(medicine_name, case=False, na=False)]
        
        if len(exact_match) > 0:
            return exact_match.iloc[0].to_dict()
        
        # ‡¶∏‡¶†‡¶ø‡¶ï ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶´‡¶æ‡¶ú‡¶ø ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
        results = self.search_medicines(medicine_name, top_k=1)
        if results:
            return results[0]
        
        return None

def main():
    st.set_page_config(
        page_title="üíä ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶Æ‡ßá‡¶°‡¶ø‡¶∏‡¶ø‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü",
        page_icon="üíä",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ‡¶ï‡¶æ‡¶∏‡ßç‡¶ü‡¶Æ CSS
    st.markdown("""
    <style>
    .main-header {
        font-size: 3.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .chat-container {
        background: linear-gradient(135deg, #f0f2f6 0%, #e6f3ff 100%);
        padding: 2rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .medicine-card {
        background: white;
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1rem 0;
        border-left: 5px solid #1f77b4;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        transition: transform 0.2s;
    }
    .medicine-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.15);
    }
    .source-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .upload-section {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 1.5rem;
        border-radius: 12px;
        margin: 1rem 0;
        border: 2px dashed #dee2e6;
    }
    .stButton > button {
        background: linear-gradient(45deg, #1f77b4, #2e86ab);
        color: white;
        border-radius: 25px;
        padding: 0.75rem 2rem;
        border: none;
        font-weight: bold;
        transition: all 0.3s;
    }
    .stButton > button:hover {
        background: linear-gradient(45deg, #2e86ab, #1f77b4);
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .search-box {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        border: 2px solid #e0e0e0;
        margin: 1rem 0;
    }
    .stats-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 0.5rem 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ‡¶π‡ßá‡¶°‡¶æ‡¶∞
    st.markdown('<h1 class="main-header">üíä ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶Æ‡ßá‡¶°‡¶ø‡¶∏‡¶ø‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü</h1>', unsafe_allow_html=True)
    st.markdown("### üéØ PDF, Word, Excel, API ‡¶∏‡¶π ‡¶∏‡¶¨ ‡¶â‡ßé‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶ì‡¶∑‡ßÅ‡¶ß‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®!")
    
    # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü ‡¶á‡¶®‡¶ø‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡ßÅ‡¶®
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = AdvancedMedicineChatbot('medicine_data.xlsx')
    
    # ‡¶∏‡¶æ‡¶á‡¶°‡¶¨‡¶æ‡¶∞
    with st.sidebar:
        st.header("üîß ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶Ö‡¶™‡¶∂‡¶®")
        
        # ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡¶•‡ßç‡¶Ø ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®
        if st.session_state.chatbot.data is not None:
            st.markdown('<div class="stats-card">', unsafe_allow_html=True)
            st.metric("üìä ‡¶Æ‡ßã‡¶ü ‡¶ì‡¶∑‡ßÅ‡¶ß", len(st.session_state.chatbot.data))
            st.markdown('</div>', unsafe_allow_html=True)
            
            st.markdown('<div class="stats-card">', unsafe_allow_html=True)
            st.metric("üìã ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ", len(st.session_state.chatbot.data.columns))
            st.markdown('</div>', unsafe_allow_html=True)
        
        # ‡¶á‡¶â‡¶®‡¶ø‡¶´‡¶æ‡¶á‡¶° ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∏‡ßá‡¶ï‡¶∂‡¶®
        st.subheader("üìÅ ‡¶∏‡¶¨ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®")
        st.markdown('<div class="upload-section">', unsafe_allow_html=True)
        
        # ‡¶ü‡ßç‡¶Ø‡¶æ‡¶¨ ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ
        tab1, tab2, tab3, tab4 = st.tabs(["üìÑ PDF", "üìù Word", "üìä Excel", "üåê API"])
        
        with tab1:
            st.write("**PDF ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®**")
            pdf_file = st.file_uploader("PDF ‡¶´‡¶æ‡¶á‡¶≤ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®", type=['pdf'], key="pdf_upload")
            if pdf_file:
                if st.button("üìÑ PDF ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®", key="add_pdf"):
                    st.session_state.chatbot.add_pdf_file(pdf_file)
        
        with tab2:
            st.write("**Word ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®**")
            word_file = st.file_uploader("Word ‡¶´‡¶æ‡¶á‡¶≤ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®", type=['docx', 'doc'], key="word_upload")
            if word_file:
                if st.button("üìù Word ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®", key="add_word"):
                    st.session_state.chatbot.add_word_file(word_file)
        
        with tab3:
            st.write("**Excel ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®**")
            excel_file = st.file_uploader("Excel ‡¶´‡¶æ‡¶á‡¶≤ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®", type=['xlsx', 'xls'], key="excel_upload")
            if excel_file:
                if st.button("üìä Excel ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®", key="add_excel"):
                    st.session_state.chatbot.add_excel_file(excel_file)
        
        with tab4:
            st.write("**API ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®**")
            api_url = st.text_input("API URL:", placeholder="https://api.example.com/medicines", key="api_url")
            api_key = st.text_input("API Key (‡¶ê‡¶ö‡ßç‡¶õ‡¶ø‡¶ï):", type="password", placeholder="your-api-key", key="api_key")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("üîó API ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®", key="add_api") and api_url:
                    st.session_state.chatbot.add_api_data(api_url, api_key)
            with col2:
                if st.button("üóëÔ∏è ‡¶∏‡¶¨ ‡¶Æ‡ßÅ‡¶õ‡ßÅ‡¶®", key="clear_all"):
                    st.session_state.chatbot.pdf_data = []
                    st.session_state.chatbot.word_data = []
                    st.session_state.chatbot.excel_data = []
                    st.session_state.chatbot.api_data = []
                    st.success("‚úÖ ‡¶∏‡¶¨ ‡¶°‡ßá‡¶ü‡¶æ ‡¶Æ‡ßÅ‡¶õ‡ßá ‡¶´‡ßá‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá")
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®
        if (st.session_state.chatbot.pdf_data or st.session_state.chatbot.word_data or 
            st.session_state.chatbot.excel_data or st.session_state.chatbot.api_data):
            st.subheader("üìã ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ‡¶´‡¶æ‡¶á‡¶≤")
            
            for pdf in st.session_state.chatbot.pdf_data:
                st.info(f"üìÑ {pdf['filename']} ({pdf['upload_time']})")
            
            for word in st.session_state.chatbot.word_data:
                st.info(f"üìù {word['filename']} ({word['upload_time']})")
            
            for excel in st.session_state.chatbot.excel_data:
                st.info(f"üìä {excel['filename']} ({excel['upload_time']})")
            
            for api in st.session_state.chatbot.api_data:
                st.info(f"üåê {api['url']} ({api['upload_time']})")
        
        # ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
        st.subheader("üîç ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®")
        specific_medicine = st.text_input("‡¶ì‡¶∑‡ßÅ‡¶ß‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®:", placeholder="‡¶Ø‡ßá‡¶Æ‡¶®: Paracetamol")
        
        col1, col2 = st.columns(2)
        with col1:
            search_specific = st.button("üîç ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®")
        with col2:
            clear_specific = st.button("üóëÔ∏è ‡¶Æ‡ßÅ‡¶õ‡ßÅ‡¶®")
        
        if search_specific and specific_medicine:
            result = st.session_state.chatbot.get_medicine_details(specific_medicine)
            if result:
                st.success("‚úÖ ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ó‡ßá‡¶õ‡ßá!")
                st.markdown('<div class="medicine-card">', unsafe_allow_html=True)
                for key, value in result.items():
                    if key not in ['combined_text', 'cleaned_text', 'similarity_score']:
                        st.write(f"**{key}:** {value}")
                st.markdown('</div>', unsafe_allow_html=True)
            else:
                st.warning("‚ùå ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø")
        
        if clear_specific:
            st.rerun()
    
    # ‡¶Æ‡ßÇ‡¶≤ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶´‡ßá‡¶∏
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    
    # ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö ‡¶ü‡¶æ‡¶á‡¶™ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®
    search_type = st.radio(
        "üîç ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö ‡¶ü‡¶æ‡¶á‡¶™ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®:",
        ["üìä ‡¶Æ‡ßÇ‡¶≤ Excel ‡¶°‡ßá‡¶ü‡¶æ", "üåê ‡¶∏‡¶¨ ‡¶â‡ßé‡¶∏ (PDF/Word/Excel/API)"],
        horizontal=True
    )
    
    # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡¶∞‡¶ø
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡¶∞‡¶ø ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®
    for message in st.session_state.chat_history:
        if message['type'] == 'user':
            st.markdown(f"**üë§ ‡¶Ü‡¶™‡¶®‡¶ø:** {message['content']}")
        else:
            st.markdown(f"**ü§ñ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü:** {message['content']}")
    
    # ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ‡¶∞ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü
    st.markdown('<div class="search-box">', unsafe_allow_html=True)
    user_query = st.text_input("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®:", placeholder="‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶ú‡ßç‡¶¨‡¶∞‡ßá‡¶∞ ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶ï‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶õ‡ßá?")
    st.markdown('</div>', unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        search_button = st.button("üîç ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®")
    with col2:
        clear_button = st.button("üóëÔ∏è ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶Æ‡ßÅ‡¶õ‡ßÅ‡¶®")
    with col3:
        st.markdown("üí° **‡¶™‡¶∞‡¶æ‡¶Æ‡¶∞‡ßç‡¶∂:** ‡¶ì‡¶∑‡ßÅ‡¶ß‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ, ‡¶â‡¶™‡¶ï‡¶æ‡¶∞‡¶ø‡¶§‡¶æ, ‡¶™‡¶æ‡¶∞‡ßç‡¶∂‡ßç‡¶¨‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶∏‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®")
    
    if clear_button:
        st.session_state.chat_history = []
        st.rerun()
    
    if search_button and user_query:
        # ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ‡¶∞ ‡¶¨‡¶æ‡¶∞‡ßç‡¶§‡¶æ ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡¶∞‡¶ø‡¶§‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
        st.session_state.chat_history.append({
            'type': 'user',
            'content': user_query
        })
        
        if search_type == "üìä ‡¶Æ‡ßÇ‡¶≤ Excel ‡¶°‡ßá‡¶ü‡¶æ":
            # ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶Æ‡ßÇ‡¶≤ Excel ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö
            results = st.session_state.chatbot.search_medicines(user_query, top_k=3)
            
            if results:
                response = "üîç ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§ ‡¶ì‡¶∑‡ßÅ‡¶ß‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ó‡ßá‡¶õ‡ßá:\n\n"
                
                for i, result in enumerate(results, 1):
                    response += f"**{i}. {result.get(list(result.keys())[0], 'Unknown Medicine')}**\n"
                    
                    # ‡¶Æ‡ßÇ‡¶≤ ‡¶§‡¶•‡ßç‡¶Ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
                    for key, value in result.items():
                        if key not in ['combined_text', 'cleaned_text', 'similarity_score'] and pd.notna(value) and str(value).strip():
                            response += f"   ‚Ä¢ **{key}:** {value}\n"
                    
                    response += f"   ‚Ä¢ **‡¶Æ‡¶ø‡¶≤‡ßá‡¶∞ ‡¶π‡¶æ‡¶∞:** {result.get('similarity_score', 0):.1%}\n\n"
            else:
                response = """‚ùå ‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶è‡¶Æ‡¶® ‡¶ï‡ßã‡¶® ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø‡•§

üí° **‡¶™‡¶∞‡¶æ‡¶Æ‡¶∞‡ßç‡¶∂:**
‚Ä¢ ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
‚Ä¢ ‡¶ì‡¶∑‡ßÅ‡¶ß‡ßá‡¶∞ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶®‡¶æ‡¶Æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
‚Ä¢ ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶¨‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶â‡¶≠‡¶Ø‡¶º ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®"""
        
        else:
            # ‡¶∏‡¶¨ ‡¶â‡ßé‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö
            results = st.session_state.chatbot.search_all_sources(user_query, top_k=5)
            
            if results:
                response = "üîç ‡¶∏‡¶¨ ‡¶â‡ßé‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶´‡¶≤‡¶æ‡¶´‡¶≤:\n\n"
                
                for i, result in enumerate(results, 1):
                    source_icon = {
                        'Main Excel': 'üìä',
                        'Uploaded Excel': 'üìä',
                        'PDF': 'üìÑ',
                        'Word': 'üìù',
                        'API': 'üåê'
                    }.get(result['source'], 'üìÑ')
                    
                    response += f"**{i}. {source_icon} {result['source']} - {result['filename']}**\n"
                    response += f"   ‚Ä¢ **‡¶∏‡ßç‡¶ï‡ßã‡¶∞:** {result['score']:.1%}\n"
                    response += f"   ‚Ä¢ **‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü:** {result['context']}\n"
                    
                    if result['data']:
                        response += "   ‚Ä¢ **‡¶§‡¶•‡ßç‡¶Ø:**\n"
                        for key, value in result['data'].items():
                            if pd.notna(value) and str(value).strip():
                                response += f"     - {key}: {value}\n"
                    
                    response += "\n"
            else:
                response = """‚ùå ‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶ï‡ßã‡¶® ‡¶â‡ßé‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶è‡¶Æ‡¶® ‡¶§‡¶•‡ßç‡¶Ø ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø‡•§

üí° **‡¶™‡¶∞‡¶æ‡¶Æ‡¶∞‡ßç‡¶∂:**
‚Ä¢ PDF, Word, Excel ‡¶´‡¶æ‡¶á‡¶≤ ‡¶¨‡¶æ API ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
‚Ä¢ ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
‚Ä¢ ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶¨‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶â‡¶≠‡¶Ø‡¶º ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®"""
        
        # ‡¶¨‡¶ü‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡¶∞‡¶ø‡¶§‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
        st.session_state.chat_history.append({
            'type': 'bot',
            'content': response
        })
        
        st.rerun()
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ‡¶´‡ßÅ‡¶ü‡¶æ‡¶∞
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**üíä ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø ‡¶∏‡ßÅ‡¶∞‡¶ï‡ßç‡¶∑‡¶æ**")
        st.markdown("‡¶∏‡¶†‡¶ø‡¶ï ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®")
    with col2:
        st.markdown("**üîç ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö**")
        st.markdown("‡¶∏‡¶¨ ‡¶â‡ßé‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶§‡¶•‡ßç‡¶Ø")
    with col3:
        st.markdown("**üåê ‡¶¨‡¶π‡ßÅ ‡¶â‡ßé‡¶∏ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®**")
        st.markdown("PDF, Word, Excel, API ‡¶∏‡¶π")

if __name__ == "__main__":
    main()
